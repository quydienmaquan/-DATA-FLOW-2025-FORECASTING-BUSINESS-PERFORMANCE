{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14508,
     "status": "ok",
     "timestamp": 1740340952630,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "dUF8xK1u0_2J"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "import optuna\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1987,
     "status": "ok",
     "timestamp": 1740340957437,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "MfNzWgdRXQbb"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"D:\\Du An\\Data FLow\\v1\\data\\train.csv\")\n",
    "test = pd.read_csv(r\"D:\\Du An\\Data FLow\\v1\\data\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740340957461,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "Y_sjLBxRYCLv"
   },
   "outputs": [],
   "source": [
    "def ensure_stationarity(df, column, max_diff=2):\n",
    "    df_adjusted = df.copy()\n",
    "    series = df[column].dropna()\n",
    "    diffs = 0\n",
    "    p_value = adfuller(series)[1]\n",
    "\n",
    "    print(f\"\\nChecking stationarity for {column}:\")\n",
    "    print(f\"Initial p-value: {p_value:.4f}\")\n",
    "\n",
    "    while p_value > 0.05 and diffs < max_diff:\n",
    "        series = series.diff().dropna()\n",
    "        df_adjusted[column] = df_adjusted[column].diff()\n",
    "        diffs += 1\n",
    "        p_value = adfuller(series)[1] if len(series) > 0 else 1.0\n",
    "        print(f\"After diff {diffs}: p-value = {p_value:.4f}\")\n",
    "\n",
    "    if diffs == max_diff and p_value > 0.05:\n",
    "        print(f\"Warning: {column} still non-stationary after {max_diff} differences\")\n",
    "    else:\n",
    "        print(f\"{column} is stationary after {diffs} differences\")\n",
    "\n",
    "    df_adjusted = df_adjusted.dropna().fillna(method='ffill')\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740340957514,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "7bzgxQVTYER7"
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"{model_name} Results:\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740340957518,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "iTbpU7ZQYF_v"
   },
   "outputs": [],
   "source": [
    "def plot_results(y_true, y_pred, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_true, label='Thực tế', color='blue')\n",
    "    plt.plot(y_pred, label=model_name, linestyle='--', color='orange')\n",
    "    plt.title(f'Dự báo Doanh thu với {model_name}')\n",
    "    plt.xlabel('Thời gian')\n",
    "    plt.ylabel('Doanh thu')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y291QwWGYH-e"
   },
   "source": [
    "# --- Model 1: LSTM ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1740340957565,
     "user": {
      "displayName": "Ma Quân",
      "userId": "09295775869266863311"
     },
     "user_tz": -420
    },
    "id": "obyJDzUaYIuw"
   },
   "outputs": [],
   "source": [
    "def run_lstm():\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    print(\"\\n=== LSTM Model ===\")\n",
    "    train_stationary = ensure_stationarity(train, 'Revenue')\n",
    "    test_stationary = ensure_stationarity(test, 'Revenue')\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_stationary[['Revenue']])\n",
    "    test_scaled = scaler.transform(test_stationary[['Revenue']])\n",
    "\n",
    "    def create_sequences(data, seq_length=10):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:i + seq_length])\n",
    "            y.append(data[i + seq_length, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    seq_length = 10\n",
    "    X_train, y_train = create_sequences(train_scaled, seq_length)\n",
    "    X_test, y_test = create_sequences(test_scaled, seq_length)\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
    "              callbacks=[EarlyStopping(patience=5, restore_best_weights=True)], verbose=1)\n",
    "\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled)[:, 0]\n",
    "    y_true = test_stationary['Revenue'][seq_length:]\n",
    "\n",
    "    y_pred = evaluate(y_true, y_pred, \"LSTM\")\n",
    "    plot_results(y_true, y_pred, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoFOZ6lfYLm4",
    "outputId": "874e1dae-e0ae-4647-e297-e4fa0335311a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LSTM Model ===\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 812. MiB for an array with shape (901677, 118) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_lstm()\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36mrun_lstm\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== LSTM Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m train_stationary \u001b[38;5;241m=\u001b[39m ensure_stationarity(train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m test_stationary \u001b[38;5;241m=\u001b[39m ensure_stationarity(test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mensure_stationarity\u001b[1;34m(df, column, max_diff)\u001b[0m\n\u001b[0;32m      3\u001b[0m series \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      4\u001b[0m diffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m p_value \u001b[38;5;241m=\u001b[39m adfuller(series)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChecking stationarity for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\stattools.py:304\u001b[0m, in \u001b[0;36madfuller\u001b[1;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag must be less than (nobs/2 - 1 - ntrend) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere n trend is the number of included \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeterministic regressors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    303\u001b[0m xdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiff(x)\n\u001b[1;32m--> 304\u001b[0m xdall \u001b[38;5;241m=\u001b[39m lagmat(xdiff[:, \u001b[38;5;28;01mNone\u001b[39;00m], maxlag, trim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m, original\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    305\u001b[0m nobs \u001b[38;5;241m=\u001b[39m xdall\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    307\u001b[0m xdall[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39mnobs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# replace 0 xdiff with level of x\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\tsatools.py:398\u001b[0m, in \u001b[0;36mlagmat\u001b[1;34m(x, maxlag, trim, original, use_pandas)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlag \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m nobs:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag should be < nobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 398\u001b[0m lm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nobs \u001b[38;5;241m+\u001b[39m maxlag, nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    400\u001b[0m     lm[\n\u001b[0;32m    401\u001b[0m     maxlag \u001b[38;5;241m-\u001b[39m k: nobs \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m-\u001b[39m k,\n\u001b[0;32m    402\u001b[0m     nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m-\u001b[39m k): nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m-\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    403\u001b[0m     ] \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 812. MiB for an array with shape (901677, 118) and data type float64"
     ]
    }
   ],
   "source": [
    "run_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K0AjgYbYPE_"
   },
   "source": [
    "# --- Model 2: ARIMA ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GTMmOSnSYSlu"
   },
   "outputs": [],
   "source": [
    "def run_arima():\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "    print(\"\\n=== ARIMA Model ===\")\n",
    "    train_stationary = ensure_stationarity(train, 'Revenue')\n",
    "    test_stationary = ensure_stationarity(test, 'Revenue')\n",
    "\n",
    "    model = ARIMA(train_stationary['Revenue'], order=(1, 1, 1)).fit()\n",
    "    y_pred = model.forecast(steps=len(test_stationary))\n",
    "    y_true = test_stationary['Revenue']\n",
    "\n",
    "    y_pred = evaluate(y_true, y_pred, \"ARIMA\")\n",
    "    plot_results(y_true, y_pred, \"ARIMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xMHNVxjoYWEz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ARIMA Model ===\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 812. MiB for an array with shape (901677, 118) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_arima()\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mrun_arima\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARIMA\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ARIMA Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m train_stationary \u001b[38;5;241m=\u001b[39m ensure_stationarity(train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m test_stationary \u001b[38;5;241m=\u001b[39m ensure_stationarity(test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m ARIMA(train_stationary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m], order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mfit()\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mensure_stationarity\u001b[1;34m(df, column, max_diff)\u001b[0m\n\u001b[0;32m      3\u001b[0m series \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m      4\u001b[0m diffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m p_value \u001b[38;5;241m=\u001b[39m adfuller(series)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChecking stationarity for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\stattools.py:304\u001b[0m, in \u001b[0;36madfuller\u001b[1;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag must be less than (nobs/2 - 1 - ntrend) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere n trend is the number of included \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeterministic regressors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    303\u001b[0m xdiff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiff(x)\n\u001b[1;32m--> 304\u001b[0m xdall \u001b[38;5;241m=\u001b[39m lagmat(xdiff[:, \u001b[38;5;28;01mNone\u001b[39;00m], maxlag, trim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m, original\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    305\u001b[0m nobs \u001b[38;5;241m=\u001b[39m xdall\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    307\u001b[0m xdall[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39mnobs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# replace 0 xdiff with level of x\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\tsatools.py:398\u001b[0m, in \u001b[0;36mlagmat\u001b[1;34m(x, maxlag, trim, original, use_pandas)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlag \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m nobs:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxlag should be < nobs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 398\u001b[0m lm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nobs \u001b[38;5;241m+\u001b[39m maxlag, nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(maxlag \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    400\u001b[0m     lm[\n\u001b[0;32m    401\u001b[0m     maxlag \u001b[38;5;241m-\u001b[39m k: nobs \u001b[38;5;241m+\u001b[39m maxlag \u001b[38;5;241m-\u001b[39m k,\n\u001b[0;32m    402\u001b[0m     nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m-\u001b[39m k): nvar \u001b[38;5;241m*\u001b[39m (maxlag \u001b[38;5;241m-\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    403\u001b[0m     ] \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 812. MiB for an array with shape (901677, 118) and data type float64"
     ]
    }
   ],
   "source": [
    "run_arima()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkAjce48YYm-"
   },
   "source": [
    "# --- Model 3: XGBoost ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qRvRNEYYZDK"
   },
   "outputs": [],
   "source": [
    "def run_xgboost():\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    print(\"\\n=== XGBoost Model ===\")\n",
    "    train_stationary = ensure_stationarity(train, 'Revenue')\n",
    "    test_stationary = ensure_stationarity(test, 'Revenue')\n",
    "    train_stationary_units = ensure_stationarity(train, 'Units')\n",
    "    test_stationary_units = ensure_stationarity(test, 'Units')\n",
    "\n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    model.fit(train_stationary_units[['Units']], train_stationary['Revenue'])\n",
    "    y_pred = model.predict(test_stationary_units[['Units']])\n",
    "    y_true = test_stationary['Revenue']\n",
    "\n",
    "    y_pred = evaluate(y_true, y_pred, \"XGBoost\")\n",
    "    plot_results(y_true, y_pred, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UenizjpYchH"
   },
   "outputs": [],
   "source": [
    "run_xgboost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBl4BVtlYe8W"
   },
   "source": [
    "# --- Model 4: PatchTST (Transformer) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RJxWhZ9Yf7S"
   },
   "outputs": [],
   "source": [
    "def run_patchtst():\n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import PatchTST\n",
    "\n",
    "    print(\"\\n=== PatchTST Model ===\")\n",
    "    train_stationary = ensure_stationarity(train, 'Revenue')\n",
    "    test_stationary = ensure_stationarity(test, 'Revenue')\n",
    "\n",
    "    train_df = pd.DataFrame({\n",
    "        'ds': pd.date_range(start='2020-01-01', periods=len(train_stationary), freq='D'),  # Adjust start date\n",
    "        'y': train_stationary['Revenue'],\n",
    "        'unique_id': '1'\n",
    "    })\n",
    "    test_df = pd.DataFrame({\n",
    "        'ds': pd.date_range(start=train_df['ds'].iloc[-1] + pd.Timedelta(days=1),\n",
    "                           periods=len(test_stationary), freq='D'),\n",
    "        'unique_id': '1'\n",
    "    })\n",
    "\n",
    "    nf = NeuralForecast(models=[PatchTST(input_size=10, h=1, max_steps=500)], freq='D')\n",
    "    nf.fit(df=train_df)\n",
    "    y_pred = nf.predict(df=test_df)['PatchTST'].values\n",
    "    y_true = test_stationary['Revenue']\n",
    "\n",
    "    y_pred = evaluate(y_true, y_pred, \"PatchTST\")\n",
    "    plot_results(y_true, y_pred, \"PatchTST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AljcNETkYjM_"
   },
   "outputs": [],
   "source": [
    "run_patchtst()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g8cvkjzYm5m"
   },
   "source": [
    "# --- Model 5: Prophet ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbmbzQiCYqas"
   },
   "outputs": [],
   "source": [
    "def run_prophet():\n",
    "    from prophet import Prophet\n",
    "\n",
    "    print(\"\\n=== Prophet Model ===\")\n",
    "    train_stationary = ensure_stationarity(train, 'Revenue')\n",
    "    test_stationary = ensure_stationarity(test, 'Revenue')\n",
    "\n",
    "    train_df = pd.DataFrame({\n",
    "        'ds': pd.date_range(start='2020-01-01', periods=len(train_stationary), freq='D'),  # Adjust start date\n",
    "        'y': train_stationary['Revenue']\n",
    "    })\n",
    "\n",
    "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)\n",
    "    model.fit(train_df)\n",
    "    future = model.make_future_dataframe(periods=len(test_stationary), freq='D')\n",
    "    forecast = model.predict(future)\n",
    "    y_pred = forecast['yhat'][-len(test_stationary):].values\n",
    "    y_true = test_stationary['Revenue']\n",
    "\n",
    "    y_pred = evaluate(y_true, y_pred, \"Prophet\")\n",
    "    plot_results(y_true, y_pred, \"Prophet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K35VqIleYrhP"
   },
   "outputs": [],
   "source": [
    "run_prophet()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMB3ZaNy2TT2Nkir4PWn6tP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
